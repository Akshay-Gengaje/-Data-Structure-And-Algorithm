# Big O Notation

Big O notation is used in Computer Science to describe the performance or complexity of an algorithm. It specifically measures the worst-case scenario, or the maximum amount of time, space, or other resources that an algorithm will require.

## Time Complexity

Time complexity of an algorithm quantifies the amount of time taken by an algorithm to run, as a function of the size of the input to the program. Here are some common time complexities:

- **O(1)**: Constant time complexity. The running time of the algorithm is constant and does not grow with the size of the input.

- **O(n)**: Linear time complexity. The running time of the algorithm grows linearly with the size of the input.

- **O(n^2)**: Quadratic time complexity. The running time of the algorithm is proportional to the square of the size of the input.

- **O(log n)**: Logarithmic time complexity. The running time of the algorithm grows logarithmically with the size of the input.

## Space Complexity

Space complexity of an algorithm quantifies the amount of space or memory taken by an algorithm to run, as a function of the size of the input to the program. It's similar to time complexity, but instead of measuring time, we measure space.

## Conclusion

Understanding Big O notation helps in analyzing algorithms for efficiency. It provides us with a high-level understanding of which algorithms are scalable and efficient in terms of time and space.

